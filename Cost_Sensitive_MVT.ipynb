{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foraging_sim.py\n",
    "# Explorer → Forager on a shared DataFrame; no 'unknown' anywhere.\n",
    "# Explorer reveals mines via per-mine 'revealed' flags; auto-labels when fully revealed / empty.\n",
    "# Forager softmax decisions + animations that reflect live, shared env.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from matplotlib import colors as mcolors\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# ------------------- Color map for environment richness -------------------\n",
    "RICHNESS_COLORS = {\n",
    "    \"poor\": \"sandybrown\",\n",
    "    \"neutral\": \"lightgreen\",\n",
    "    \"rich\": \"gold\",\n",
    "}\n",
    "\n",
    "# ------------------- Environment initializer (TRUE has no 'unknown') -------------------\n",
    "def init_gridworld(\n",
    "    size: int = 3,\n",
    "    seed: Optional[int] = None,\n",
    "    # TRUE overall richness distribution\n",
    "    p_overall: List[float] = (0.10, 0.35, 0.55),  # poor, neutral, rich\n",
    "    # P(#mines = 0..3)\n",
    "    p_mines: List[float] = (0.25, 0.40, 0.25, 0.10),\n",
    "    # Optional preset visible map: dict[(row,col)->label] to override labels (e.g., a known map)\n",
    "    preset_visible_overall: Optional[Dict[Tuple[int,int], str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds an N×N grid with:\n",
    "      - TRUE hidden fields: overall∈{poor,neutral,rich}, mines∈{poor,neutral,rich} or None\n",
    "      - VISIBLE fields start labeled (from preset if provided, else from TRUE).\n",
    "      - Mines start HIDDEN using explicit 'Mine i revealed' flags (False where a TRUE mine exists).\n",
    "        Visible mine fields are None until revealed by the explorer.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed if seed is not None else None)\n",
    "\n",
    "    true_overall_labels = np.array([\"poor\", \"neutral\", \"rich\"])\n",
    "    p_overall = np.asarray(p_overall, dtype=float)\n",
    "    p_overall = p_overall / p_overall.sum()\n",
    "\n",
    "    p_mines = np.asarray(p_mines, dtype=float); p_mines /= p_mines.sum()\n",
    "\n",
    "    mine_cat_given_overall = {\n",
    "        \"poor\":    {\"poor\": 0.70, \"neutral\": 0.25, \"rich\": 0.05},\n",
    "        \"neutral\": {\"poor\": 0.25, \"neutral\": 0.50, \"rich\": 0.25},\n",
    "        \"rich\":    {\"poor\": 0.10, \"neutral\": 0.30, \"rich\": 0.60},\n",
    "    }\n",
    "    reward_prob_map = {\"poor\": 0.20, \"neutral\": 0.50, \"rich\": 0.80}\n",
    "\n",
    "    def sample_k(ov: str) -> int:\n",
    "        base = p_mines.copy()\n",
    "        if ov == \"rich\":\n",
    "            base[:2] = 0.0   # at least 2 mines\n",
    "        elif ov == \"poor\":\n",
    "            base[3] = 0.0   # cannot have 3 mines\n",
    "        s = base.sum()\n",
    "        if s == 0:\n",
    "            return 2 if ov == \"rich\" else (1 if ov == \"poor\" else 0)\n",
    "        base /= s\n",
    "        return int(rng.choice([0, 1, 2, 3], p=base))\n",
    "\n",
    "    def sample_mine_cat(ov: str, k: int) -> List[Optional[str]]:\n",
    "        if k == 0:\n",
    "            return [None, None, None]\n",
    "        probs_map = mine_cat_given_overall[ov]\n",
    "        cats = np.array([\"poor\", \"neutral\", \"rich\"])\n",
    "        probs = np.array([probs_map[\"poor\"], probs_map[\"neutral\"], probs_map[\"rich\"]], dtype=float)\n",
    "        probs = probs / probs.sum()\n",
    "        sampled = rng.choice(cats, size=k, p=probs).tolist()\n",
    "        return (sampled + [None] * (3 - k))[:3]\n",
    "\n",
    "    digs_range = {\"poor\": (1, 2), \"neutral\": (2, 4), \"rich\": (3, 6)}\n",
    "    def digs_allowed_for(cat: Optional[str]) -> Optional[int]:\n",
    "        if cat is None: return None\n",
    "        low, high = digs_range[cat]\n",
    "        return int(rng.integers(low, high + 1))\n",
    "\n",
    "    N = size\n",
    "    center = (N // 2, N // 2)\n",
    "    rows = []\n",
    "\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            is_center = (r, c) == center\n",
    "            if is_center:\n",
    "                overall_true = \"poor\"   # base is empty poor\n",
    "                t1 = t2 = t3 = None\n",
    "                rp1 = rp2 = rp3 = None\n",
    "                d1 = d2 = d3 = None\n",
    "                env_empty_true = True\n",
    "            else:\n",
    "                overall_true = rng.choice(true_overall_labels, p=p_overall).item()\n",
    "                k = sample_k(overall_true)\n",
    "                t1, t2, t3 = sample_mine_cat(overall_true, k)\n",
    "                rp1 = reward_prob_map[t1] if t1 else None\n",
    "                rp2 = reward_prob_map[t2] if t2 else None\n",
    "                rp3 = reward_prob_map[t3] if t3 else None\n",
    "                d1 = digs_allowed_for(t1)\n",
    "                d2 = digs_allowed_for(t2)\n",
    "                d3 = digs_allowed_for(t3)\n",
    "                env_empty_true = (k == 0)\n",
    "\n",
    "            rows.append({\n",
    "                \"Location\": f\"{r}:{c}\", \"Row\": r, \"Col\": c, \"is_center\": is_center,\n",
    "\n",
    "                # TRUE (no 'unknown' anywhere)\n",
    "                \"TRUE Number 1 mines\": t1,\n",
    "                \"TRUE Number 2 mines\": t2,\n",
    "                \"TRUE Number 3 mines\": t3,\n",
    "                \"TRUE Mine 1 reward_prob\": rp1,\n",
    "                \"TRUE Mine 2 reward_prob\": rp2,\n",
    "                \"TRUE Mine 3 reward_prob\": rp3,\n",
    "                \"TRUE Mine 1 digs_allowed\": d1,\n",
    "                \"TRUE Mine 2 digs_allowed\": d2,\n",
    "                \"TRUE Mine 3 digs_allowed\": d3,\n",
    "                \"TRUE env_empty\": env_empty_true,\n",
    "                \"TRUE Overall Richness\": overall_true,\n",
    "\n",
    "                # VISIBLE initial mines are hidden (revealed flags False where a TRUE mine exists)\n",
    "                \"Number 1 mines\": None,\n",
    "                \"Number 2 mines\": None,\n",
    "                \"Number 3 mines\": None,\n",
    "                \"Mine 1 reward_prob\": None,\n",
    "                \"Mine 2 reward_prob\": None,\n",
    "                \"Mine 3 reward_prob\": None,\n",
    "                \"Mine 1 digs_allowed\": None,\n",
    "                \"Mine 2 digs_allowed\": None,\n",
    "                \"Mine 3 digs_allowed\": None,\n",
    "\n",
    "                \"Mine 1 revealed\": (t1 is None),\n",
    "                \"Mine 2 revealed\": (t2 is None),\n",
    "                \"Mine 3 revealed\": (t3 is None),\n",
    "\n",
    "                # Runtime\n",
    "                \"Overall Richness of this environment\": None,  # set below\n",
    "                \"Explorer Label\": None, \n",
    "                \"env_empty\": env_empty_true,\n",
    "                \"visited_by_explorer\": False,\n",
    "            })\n",
    "\n",
    "    env = pd.DataFrame(rows)\n",
    "\n",
    "    # --- Visible overall richness: use PRESET if provided; otherwise TRUE ---\n",
    "    if preset_visible_overall:\n",
    "        env[\"Overall Richness of this environment\"] = env[\"TRUE Overall Richness\"]\n",
    "        for (rr, cc), lab in preset_visible_overall.items():\n",
    "            env.loc[(env[\"Row\"] == rr) & (env[\"Col\"] == cc),\n",
    "                    \"Overall Richness of this environment\"] = lab\n",
    "    else:\n",
    "        env[\"Overall Richness of this environment\"] = env[\"TRUE Overall Richness\"]\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a108097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Runtime helpers (operate IN-PLACE on the shared env) -------------------\n",
    "def ensure_runtime_columns(env: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Keep digs_remaining columns\n",
    "    for i in (1, 2, 3):\n",
    "        digs_col = f\"Mine {i} digs_allowed\"\n",
    "        rem_col  = f\"Mine {i} digs_remaining\"\n",
    "        if rem_col not in env.columns:\n",
    "            env[rem_col] = env[digs_col]\n",
    "        else:\n",
    "            mask = env[rem_col].isna() & env[digs_col].notna()\n",
    "            env.loc[mask, rem_col] = env.loc[mask, digs_col]\n",
    "\n",
    "    # Ensure revealed flags exist (if missing, infer from visible mine fields)\n",
    "    for i in (1, 2, 3):\n",
    "        rev_col = f\"Mine {i} revealed\"\n",
    "        if rev_col not in env.columns:\n",
    "            # revealed if visible category is not None OR TRUE mine absent\n",
    "            #env[rev_col] = env[f\"Number {i} mines\"].notna() | env[f\"TRUE Number {i} mines\"].isna()\n",
    "            env[rev_col] = False\n",
    "\n",
    "    if \"visited_by_explorer\" not in env.columns:\n",
    "        env[\"visited_by_explorer\"] = False\n",
    "    if \"env_empty\" not in env.columns:\n",
    "        env[\"env_empty\"] = False\n",
    "    return env  # same object\n",
    "\n",
    "def _indexify(env: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure we have a MultiIndex on (Row, Col) while KEEPING columns.\"\"\"\n",
    "    if list(env.index.names) != [\"Row\", \"Col\"]:\n",
    "        env.set_index([\"Row\", \"Col\"], inplace=True, drop=False)\n",
    "    return env\n",
    "\n",
    "def _cell_total_remaining_digs_df(env_idx: pd.DataFrame, pos: Tuple[int, int]) -> int:\n",
    "    s = 0\n",
    "    for i in (1, 2, 3):\n",
    "        rem = env_idx.loc[pos, f\"Mine {i} digs_remaining\"]\n",
    "        if pd.notna(rem): s += int(float(rem))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48330f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Leader -------------------\n",
    "@dataclass\n",
    "class LeaderConfig:\n",
    "    total_food: int = 200 #leader_set_explorer\n",
    "    #if past one round - team_reward = ExplorerAgent.total_food_left() + MVTAgent.total_food_left() + MVTAgent.total_reward\n",
    "    alpha: float = 0.3\n",
    "\n",
    "class LeaderAgent:\n",
    "    \"\"\"\n",
    "    - Random tradeoff of the total_food to explorer and forager\n",
    "    - Based on coins collected after the round * learning_rate, allocated differently proportionally\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: LeaderConfig, seed: Optional[int] = None):\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(seed if seed is not None else None)\n",
    "\n",
    "        self.threshold = True \n",
    "        self.resource_tradeoff = [0.5 * self.cfg.total_food, 0.5 * self.cfg.total_food]\n",
    "\n",
    "     #def tradeoff(self):\n",
    "        #randomly selected\n",
    "        #proportions = np.random.rand(1,2)\n",
    "\n",
    "         #start with half-half...\n",
    "    \n",
    "    def set_forager(self, forager: \"MVTAgent\"):\n",
    "        self.forager = forager\n",
    "        \n",
    "    def update_allocation(self): #after each game\n",
    "        #the leader should increase forage resouces because explorer is based on luck \n",
    "        #perceived_reward = self.forager.total_reward (if round 1) or self.team_reward (if more than one round) * self.cfg.alpha \n",
    "        explorer_resources, forager_resources = self.resource_tradeoff\n",
    "        perceived_reward = self.forager.total_food_left()\n",
    "        diff_in_rewards = perceived_reward - self.forager.total_reward\n",
    "\n",
    "        change = (self.cfg.alpha * diff_in_rewards) / self.forager.total_reward\n",
    "\n",
    "        explorer_resources *= (1 - change)\n",
    "        forager_resources *= (1 + change)\n",
    "\n",
    "        total = explorer_resources + forager_resources\n",
    "        scale = self.cfg.total_food / total\n",
    "        explorer_resources *= scale\n",
    "        forager_resources *= scale\n",
    "     \n",
    "        self.resource_tradeoff = [explorer_resources, forager_resources]\n",
    "        return self.resource_tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assumed available from your codebase:\n",
    "# - ensure_runtime_columns(env_df): adds runtime cols like visited_by_explorer, env_empty, etc.\n",
    "# - _indexify(env_df): ensures MultiIndex by (row, col)\n",
    "\n",
    "# ------------------- Explorer (count-only: #mines & revealed flags) + CSV logging -------------------\n",
    "@dataclass\n",
    "class ExplorerConfig:\n",
    "    init_resource: int = 100\n",
    "    move_cost: int = 4\n",
    "    scan_cost: int = 2\n",
    "    gamma: float = 0.1\n",
    "    beta_local: float = 0.6     # weight on local hidden-mine pressure\n",
    "    beta_global: float = 0.4    # weight on global hidden-mine pressure\n",
    "    avoid_base: bool = True\n",
    "    no_backtrack: bool = True   # never revisit an already visited block\n",
    "    cost_sensitive_index: int = 0.1\n",
    "\n",
    "class ExplorerAgent:\n",
    "    \"\"\"\n",
    "    COUNT-ONLY EXPLORER (CSV-logging version)\n",
    "    - Decisions depend ONLY on:\n",
    "        (1) how many TRUE mine slots exist on tiles, and\n",
    "        (2) whether each slot is revealed (Mine i revealed == True/False).\n",
    "    - Single scan per cell (reveals ONE hidden mine slot).\n",
    "    - No backtracking (cannot enter a previously visited cell).\n",
    "    - Opens tiles for the Forager by setting env['visited_by_explorer'] = True.\n",
    "    - Auto-label rules:\n",
    "        * After first visit: if tile truly has NO mines -> label 'poor' + mark all three as revealed.\n",
    "        * When ALL TRUE mines at the tile are revealed -> set label to TRUE overall and mark all revealed.\n",
    "    - Logging:\n",
    "        * Every step logs action, decision values, feasibility flags, resources, position, counts, and config.\n",
    "        * Call run(..., csv_path=\"explorer_round1.csv\") to persist the log as CSV.\n",
    "    - Transfer map to Forager: export_env_for_forager() returns the SAME env reference.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_df: pd.DataFrame, cfg: ExplorerConfig, seed: Optional[int] = None):\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(seed if seed is not None else None)\n",
    "\n",
    "        self.env = ensure_runtime_columns(env_df)\n",
    "        _indexify(self.env)\n",
    "\n",
    "        self.resource = cfg.init_resource\n",
    "        size = int(self.env.index.get_level_values(0).max()) + 1\n",
    "        self.base = (size // 2, size // 2)\n",
    "        self.pos = self.base\n",
    "        self.left_base_once = False\n",
    "        self.t = 0\n",
    "        self.total_moves = 0\n",
    "        self.log: List[Dict] = []\n",
    "\n",
    "        # Open base for the forager and possibly autolabel\n",
    "        self.env.loc[self.pos, 'visited_by_explorer'] = True\n",
    "        self._autolabel_if_ready(self.pos)\n",
    "\n",
    "        # Enforce single scan per cell\n",
    "        self.scanned_once: set[Tuple[int, int]] = set()\n",
    "\n",
    "        # Animation buffers (unchanged)\n",
    "        self.frames_pos: List[Tuple[int, int]] = []\n",
    "        self.frames_resource: List[float] = []\n",
    "        self.frames_action: List[str] = []\n",
    "        self.frames_unrevealed_mask: List[np.ndarray] = []\n",
    "        self.frames_reward_dummy: List[float] = []\n",
    "        self.frames_decision: List[str] = []\n",
    "        self._snapshot(\"start\", \"start\")\n",
    "\n",
    "    # ---- utilities ----\n",
    "    def _neighbors(self):\n",
    "        r, c = self.pos\n",
    "        neigh = [(r-1, c), (r+1, c), (r, c-1), (r, c+1)]\n",
    "        valid = [(rr, cc) for rr, cc in neigh if (rr, cc) in self.env.index]\n",
    "        if self.cfg.avoid_base and self.left_base_once:\n",
    "            valid = [p for p in valid if p != self.base]\n",
    "        if self.cfg.no_backtrack:\n",
    "            valid = [p for p in valid if not bool(self.env.loc[p, 'visited_by_explorer'])]\n",
    "        return valid\n",
    "\n",
    "    def _hidden_mines_here(self) -> List[int]:\n",
    "        \"\"\"Return mine IDs among {1,2,3} that truly exist and are not yet revealed here.\"\"\"\n",
    "        mines = []\n",
    "        for i in (1, 2, 3):\n",
    "            has_true = self.env.loc[self.pos, f\"TRUE Number {i} mines\"] is not None\n",
    "            revealed = bool(self.env.loc[self.pos, f\"Mine {i} revealed\"])\n",
    "            if has_true and (not revealed):\n",
    "                mines.append(i)\n",
    "        return mines\n",
    "\n",
    "    def _global_hidden_count(self) -> int:\n",
    "        \"\"\"Total number of hidden TRUE mine slots across the whole grid.\"\"\"\n",
    "        cnt = 0\n",
    "        for pos in self.env.index:\n",
    "            for i in (1, 2, 3):\n",
    "                has_true = self.env.loc[pos, f\"TRUE Number {i} mines\"] is not None\n",
    "                revealed = bool(self.env.loc[pos, f\"Mine {i} revealed\"])\n",
    "                if has_true and (not revealed):\n",
    "                    cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    def _fully_revealed(self, pos: Tuple[int, int]) -> bool:\n",
    "        \"\"\"Fully revealed if every TRUE mine slot at pos has revealed=True.\"\"\"\n",
    "        for i in (1, 2, 3):\n",
    "            has_true = self.env.loc[pos, f\"TRUE Number {i} mines\"] is not None\n",
    "            if has_true and (not bool(self.env.loc[pos, f\"Mine {i} revealed\"])):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _autolabel_if_ready(self, pos: Tuple[int, int]) -> None:\n",
    "        \"\"\"\n",
    "        Refresh 'Overall Richness of this environment' and 'Explorer Label' if:\n",
    "          - explorer has visited this pos, AND\n",
    "          - (a) tile has no TRUE mines -> label 'poor' and mark all revealed\n",
    "          - (b) OR all TRUE mines at the tile are revealed -> set to TRUE overall and mark all revealed.\n",
    "        \"\"\"\n",
    "        if not bool(self.env.loc[pos, \"visited_by_explorer\"]):\n",
    "            return\n",
    "\n",
    "        has_true_mine = any(self.env.loc[pos, f\"TRUE Number {i} mines\"] is not None for i in (1, 2, 3))\n",
    "        if not has_true_mine:\n",
    "            self.env.loc[pos, \"Overall Richness of this environment\"] = \"poor\"\n",
    "            self.env.loc[pos, \"Explorer Label\"] = \"poor\"\n",
    "            for i in (1, 2, 3):\n",
    "                self.env.loc[pos, f\"Mine {i} revealed\"] = True\n",
    "            return\n",
    "\n",
    "        if self._fully_revealed(pos):\n",
    "            self.env.loc[pos, \"Overall Richness of this environment\"] = self.env.loc[pos, \"TRUE Overall Richness\"]\n",
    "            self.env.loc[pos, \"Explorer Label\"] = self.env.loc[pos, \"TRUE Overall Richness\"]\n",
    "            for i in (1, 2, 3):\n",
    "                self.env.loc[pos, f\"Mine {i} revealed\"] = True\n",
    "\n",
    "    def _snapshot(self, action: str, decision: str):\n",
    "        idx = self.env\n",
    "        nrows = int(idx.index.get_level_values(0).max()) + 1\n",
    "        ncols = int(idx.index.get_level_values(1).max()) + 1\n",
    "\n",
    "        mask = np.zeros((nrows, ncols), dtype=bool)\n",
    "        for pos in idx.index:\n",
    "            r, c = pos\n",
    "            unrevealed = (not bool(idx.loc[pos, \"visited_by_explorer\"])) or any(\n",
    "                (idx.loc[pos, f\"TRUE Number {i} mines\"] is not None) and (not bool(idx.loc[pos, f\"Mine {i} revealed\"]))\n",
    "                for i in (1, 2, 3)\n",
    "            )\n",
    "            mask[r, c] = unrevealed\n",
    "\n",
    "        self.frames_unrevealed_mask.append(mask)\n",
    "        self.frames_pos.append(tuple(self.pos))\n",
    "        self.frames_resource.append(float(self.resource))\n",
    "        self.frames_action.append(action)\n",
    "        self.frames_decision.append(decision)\n",
    "        self.frames_reward_dummy.append(0.0)\n",
    "\n",
    "    # ---------- rich CSV logger ----------\n",
    "    def _log_step(\n",
    "        self,\n",
    "        *,\n",
    "        action: str,\n",
    "        decision: str,\n",
    "        resource_before: float,\n",
    "        resource_after: float,\n",
    "        v_stay: Optional[float],\n",
    "        v_leave: Optional[float],\n",
    "        can_scan_here: Optional[bool],\n",
    "        can_move: Optional[bool],\n",
    "        mine_id: Optional[int] = None,\n",
    "        revealed_cat: Optional[str] = None,\n",
    "        move_probs: Optional[List[float]] = None\n",
    "    ):\n",
    "        row = {\n",
    "            # time & pos\n",
    "            \"step\": self.t,\n",
    "            \"row\": self.pos[0],\n",
    "            \"col\": self.pos[1],\n",
    "            \"action\": action,\n",
    "            \"decision\": decision,\n",
    "\n",
    "            # resources\n",
    "            \"resource_before\": float(resource_before),\n",
    "            \"resource_after\": float(resource_after),\n",
    "\n",
    "            # decision values & feasibility\n",
    "            \"v_stay\": (None if v_stay is None else float(v_stay)),\n",
    "            \"v_leave\": (None if v_leave is None else float(v_leave)),\n",
    "            \"can_scan_here\": (None if can_scan_here is None else bool(can_scan_here)),\n",
    "            \"can_move_neighbors\": (None if can_move is None else bool(can_move)),\n",
    "\n",
    "            # counts\n",
    "            \"hidden_local\": int(len(self._hidden_mines_here())),\n",
    "            \"hidden_global\": int(self._global_hidden_count()),\n",
    "            \"total_moves\": int(self.total_moves),\n",
    "\n",
    "            # environment labels at current cell\n",
    "            \"visited_here\": bool(self.env.loc[self.pos, \"visited_by_explorer\"]),\n",
    "            \"overall_label\": str(self.env.loc[self.pos, \"Overall Richness of this environment\"]),\n",
    "            \"explorer_label\": str(self.env.loc[self.pos, \"Explorer Label\"]),\n",
    "\n",
    "            # action-specific\n",
    "            \"mine_id\": (None if mine_id is None else int(mine_id)),\n",
    "            \"revealed_cat\": (None if revealed_cat is None else str(revealed_cat)),\n",
    "            \"move_probs\": (None if move_probs is None else list(move_probs)),\n",
    "\n",
    "            # config snapshot (so the CSV is self-contained)\n",
    "            \"cfg_init_resource\": int(self.cfg.init_resource),\n",
    "            \"cfg_move_cost\": int(self.cfg.move_cost),\n",
    "            \"cfg_scan_cost\": int(self.cfg.scan_cost),\n",
    "            \"cfg_gamma\": float(self.cfg.gamma),\n",
    "            \"cfg_beta_local\": float(self.cfg.beta_local),\n",
    "            \"cfg_beta_global\": float(self.cfg.beta_global),\n",
    "            \"cfg_avoid_base\": bool(self.cfg.avoid_base),\n",
    "            \"cfg_no_backtrack\": bool(self.cfg.no_backtrack),\n",
    "        }\n",
    "        self.log.append(row)\n",
    "\n",
    "    # ---- core actions (count-only) ----\n",
    "    def _reveal_one_mine_here(self, v_stay: Optional[float], v_leave: Optional[float], can_move: bool) -> bool:\n",
    "        \"\"\"Reveal exactly ONE hidden TRUE mine slot here (count-only rule). Logs to CSV.\"\"\"\n",
    "        if self.pos in self.scanned_once:\n",
    "            return False\n",
    "        candidates = self._hidden_mines_here()\n",
    "        if not candidates or self.resource < self.cfg.scan_cost:\n",
    "            return False\n",
    "\n",
    "        resource_before = float(self.resource)\n",
    "        mine_id = int(self.rng.choice(candidates))\n",
    "        self.resource -= self.cfg.scan_cost\n",
    "\n",
    "        # Copy TRUE fields into visible for this slot\n",
    "        tcat = self.env.loc[self.pos, f\"TRUE Number {mine_id} mines\"]\n",
    "        trp  = self.env.loc[self.pos, f\"TRUE Mine {mine_id} reward_prob\"]\n",
    "        tda  = self.env.loc[self.pos, f\"TRUE Mine {mine_id} digs_allowed\"]\n",
    "\n",
    "        self.env.loc[self.pos, f\"Number {mine_id} mines\"] = tcat\n",
    "        self.env.loc[self.pos, f\"Mine {mine_id} reward_prob\"] = trp\n",
    "        self.env.loc[self.pos, f\"Mine {mine_id} digs_allowed\"] = tda\n",
    "        self.env.loc[self.pos, f\"Mine {mine_id} digs_remaining\"] = tda\n",
    "        self.env.loc[self.pos, f\"Mine {mine_id} revealed\"] = True\n",
    "\n",
    "        self.scanned_once.add(self.pos)\n",
    "\n",
    "        # Auto-label if fully revealed now (or mark poor if appropriate)\n",
    "        self._autolabel_if_ready(self.pos)\n",
    "\n",
    "        # Log & animate\n",
    "        self._log_step(\n",
    "            action=\"scan\", decision=\"scan_here\",\n",
    "            resource_before=resource_before, resource_after=float(self.resource),\n",
    "            v_stay=v_stay, v_leave=v_leave,\n",
    "            can_scan_here=True, can_move=can_move,\n",
    "            mine_id=mine_id, revealed_cat=tcat\n",
    "        )\n",
    "        self._snapshot(\"scan\", \"scan_here\")\n",
    "        return True\n",
    "\n",
    "    def _move(self, v_stay: Optional[float], v_leave: Optional[float], can_scan_here: bool) -> bool:\n",
    "        \"\"\"Move to a neighbor with count-only preference. Logs to CSV.\"\"\"\n",
    "        neigh = self._neighbors()\n",
    "        if not neigh or self.resource < self.cfg.move_cost:\n",
    "            return False\n",
    "\n",
    "        # Prefer neighbors with MORE hidden TRUE mine slots\n",
    "        eps = 1e-6\n",
    "        weights = []\n",
    "        for n in neigh:\n",
    "            hidden = sum(\n",
    "                (self.env.loc[n, f\"TRUE Number {i} mines\"] is not None) and (not bool(self.env.loc[n, f\"Mine {i} revealed\"]))\n",
    "                for i in (1, 2, 3)\n",
    "            )\n",
    "            weights.append(eps + hidden)\n",
    "        probs = np.array(weights, dtype=float)\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        choice_idx = int(self.rng.choice(len(neigh), p=probs))\n",
    "        choice = neigh[choice_idx]\n",
    "\n",
    "        # Pay & move\n",
    "        resource_before = float(self.resource)\n",
    "        self.resource -= self.cfg.move_cost\n",
    "        if (self.pos == self.base) and (choice != self.base):\n",
    "            self.left_base_once = True\n",
    "        self.pos = choice\n",
    "        self.total_moves += 1\n",
    "\n",
    "        # Open tile for forager and attempt autolabel (e.g., visited empty -> 'poor')\n",
    "        self.env.loc[self.pos, 'visited_by_explorer'] = True\n",
    "        self._autolabel_if_ready(self.pos)\n",
    "\n",
    "        # Log & animate\n",
    "        self._log_step(\n",
    "            action=\"move\", decision=\"leave\",\n",
    "            resource_before=resource_before, resource_after=float(self.resource),\n",
    "            v_stay=v_stay, v_leave=v_leave,\n",
    "            can_scan_here=can_scan_here, can_move=True,\n",
    "            move_probs=list(np.round(probs, 4))\n",
    "        )\n",
    "        self._snapshot(\"move\", \"leave\")\n",
    "        return True\n",
    "\n",
    "    # ---- decision values (count-only) ----\n",
    "    def _values(self) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        A simple, dimensionless pressure comparison based solely on counts:\n",
    "          - Local pressure ~ (#hidden here)\n",
    "          - Global pressure ~ (total #hidden elsewhere)\n",
    "        Costs only gate feasibility; they don't enter the preference directly.\n",
    "        \"\"\"\n",
    "        R_local = len(self._hidden_mines_here())           # 0..3\n",
    "        R_global = self._global_hidden_count()             # 0..(3*grid)\n",
    "        stay = self.cfg.beta_local * float(R_local)-self.cfg.scan_cost*self.cfg.gamma\n",
    "        leave = self.cfg.beta_global * float(max(R_global - R_local, 0))-self.cfg.move_cost*self.cfg.gamma\n",
    "        return float(stay), float(leave)\n",
    "\n",
    "    def step(self):\n",
    "        # If neither action is affordable, stop this phase\n",
    "        if (self.resource < self.cfg.scan_cost) and (self.resource < self.cfg.move_cost):\n",
    "            self._log_step(\n",
    "                action=\"halt\", decision=\"insufficient_resources\",\n",
    "                resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                v_stay=None, v_leave=None,\n",
    "                can_scan_here=False, can_move=False\n",
    "            )\n",
    "            self._snapshot(\"halt\", \"insufficient_resources\")\n",
    "            self.t += 1\n",
    "            return\n",
    "\n",
    "        v_stay, v_leave = self._values()\n",
    "        can_scan_here = (\n",
    "            (self.resource >= self.cfg.scan_cost)\n",
    "            and (self.pos not in self.scanned_once)\n",
    "            and (len(self._hidden_mines_here()) > 0)\n",
    "        )\n",
    "        can_move = (self.resource >= self.cfg.move_cost) and (len(self._neighbors()) > 0)\n",
    "\n",
    "        did_action = False\n",
    "        # Prefer moving if leave-pressure is higher or scanning is impossible\n",
    "        if (not can_scan_here) or (v_leave > v_stay):\n",
    "            did_action = self._move(v_stay, v_leave, can_scan_here)\n",
    "\n",
    "        # If couldn't move, try single scan here (once per cell)\n",
    "        if (not did_action) and can_scan_here:\n",
    "            did_action = self._reveal_one_mine_here(v_stay, v_leave, can_move)\n",
    "\n",
    "        # If still no action possible -> STOP PHASE (do not drain more resources)\n",
    "        if not did_action:\n",
    "            self._log_step(\n",
    "                action=\"halt\", decision=\"no_actions_left\",\n",
    "                resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                v_stay=v_stay, v_leave=v_leave,\n",
    "                can_scan_here=can_scan_here, can_move=can_move\n",
    "            )\n",
    "            self._snapshot(\"halt\", \"no_actions_left\")\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "    def total_food_left(self) -> float:\n",
    "        # Return how much of the explorer's allocated food remains.\n",
    "        return float(self.resource)\n",
    "\n",
    "    # --- Transfer the map for forager decisions (same shared DataFrame) ---\n",
    "    def export_env_for_forager(self) -> pd.DataFrame:\n",
    "        return self.env\n",
    "\n",
    "    def run(self, max_steps: int = 300, csv_path: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run explorer for up to `max_steps`. If `csv_path` is provided, save the step log to CSV.\n",
    "        Returns the pandas DataFrame of the log.\n",
    "        \"\"\"\n",
    "        for _ in range(max_steps):\n",
    "            if (self.resource < self.cfg.scan_cost) and (self.resource < self.cfg.move_cost):\n",
    "                # One final log row (if not already logged this tick)\n",
    "                self._log_step(\n",
    "                    action=\"halt\", decision=\"insufficient_resources\",\n",
    "                    resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                    v_stay=None, v_leave=None,\n",
    "                    can_scan_here=False, can_move=False\n",
    "                )\n",
    "                self._snapshot(\"halt\", \"insufficient_resources\")\n",
    "                break\n",
    "            self.step()\n",
    "            if self.log and self.log[-1].get(\"action\") == \"halt\":\n",
    "                break\n",
    "\n",
    "        df = pd.DataFrame(self.log)\n",
    "        if csv_path:\n",
    "            # Always safe-guard against nested lists (e.g., move_probs) by converting to JSON-ish strings\n",
    "            df_to_save = df.copy()\n",
    "            if \"move_probs\" in df_to_save.columns:\n",
    "                df_to_save[\"move_probs\"] = df_to_save[\"move_probs\"].apply(\n",
    "                    lambda v: (\"\" if v is None else repr(v))\n",
    "                )\n",
    "            df_to_save.to_csv(csv_path, index=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------- Helpers assumed from your codebase -------------------\n",
    "# - ensure_runtime_columns(env_df): adds runtime cols like visited_by_explorer, env_empty, etc.\n",
    "# - _indexify(env_df): ensures MultiIndex by (row, col)\n",
    "# - _cell_total_remaining_digs_df(env_df, pos): returns total remaining digs in a cell (int)\n",
    "\n",
    "# ------------------- Forager (Bayesian stay/leave + softmax neighbor move) + CSV logging -------------------\n",
    "@dataclass\n",
    "class MVTConfig:\n",
    "    # Resources & costs\n",
    "    init_resource: int = 100\n",
    "    move_cost: int = 10\n",
    "    dig_cost: int = 5\n",
    "    gamma: float = 0.1  # retained (not directly used in Bayesian core)\n",
    "    reward_amount: float = 1.0\n",
    "    avoid_base: bool = True\n",
    "\n",
    "    # Trust / mixing\n",
    "    beta_trust: float = 0.7         # weight on explorer label vs model-based E[next]\n",
    "    label_value: Dict[str, float] = None  # label→score for neighbor selection\n",
    "    env_factor: Dict[str, float] = None   # (kept for compatibility; not used in Bayesian core)\n",
    "    mine_choice_value: Dict[str, float] = None  # (kept for compatibility; can be used for within-cell mine choice)\n",
    "\n",
    "    # Temperatures\n",
    "    stay_leave_temp: float = 0.7     # softmax temperature for stay vs leave\n",
    "    move_temp: float = 0.7           # softmax temperature for neighbor choice\n",
    "    cost_sensitive_index: float=0.1\n",
    "\n",
    "    # -------- Bayesian observer (Bornstein-style) --------\n",
    "    # K discrete types; fixed means and priors; shared Normal noise variance\n",
    "    K: int = 3\n",
    "    MU: List[float] = None           # e.g., [0.2, 0.6, 0.9] * reward_amount\n",
    "    SIGMA2: float = 0.05             # observation noise variance (Normal likelihood)\n",
    "    PI: List[float] = None           # type priors, sum to 1 (e.g., [0.33, 0.33, 0.34])\n",
    "\n",
    "    # Map explorer labels to type indices (None if unrevealed/unknown)\n",
    "    label_to_type: Dict[str, Optional[int]] = None  # {'poor':0, 'neutral':1, 'rich':2}\n",
    "\n",
    "    # Baseline for leaving: 'env' uses ∑πμ; 'rate' uses running average λ\n",
    "    baseline_mode: str = \"env\"\n",
    "    ewma_eta: float = 0.1            # EWMA step for λ if baseline_mode == 'rate'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.mine_choice_value is None:\n",
    "            self.mine_choice_value = {'rich': 1.0, 'neutral': 0.6, 'poor': 0.2}\n",
    "        if self.env_factor is None:\n",
    "            self.env_factor = {'rich': 0.8, 'neutral': 0.5, 'poor': 0.2}\n",
    "        if self.label_value is None:\n",
    "            self.label_value = {'rich': 0.8, 'neutral': 0.5, 'poor': 0.2}\n",
    "        if self.MU is None:\n",
    "            # Default scaled by reward_amount (you can set explicitly)\n",
    "            self.MU = [0.2 * self.reward_amount, 0.6 * self.reward_amount, 0.9 * self.reward_amount]\n",
    "        if self.PI is None:\n",
    "            self.PI = [1.0 / self.K] * self.K\n",
    "        if self.label_to_type is None:\n",
    "            self.label_to_type = {'poor': 0, 'neutral': 1, 'rich': 2}\n",
    "        # Normalize\n",
    "        s = sum(self.PI)\n",
    "        if s <= 0:\n",
    "            raise ValueError(\"PI must contain positive entries.\")\n",
    "        self.PI = [p / s for p in self.PI]\n",
    "\n",
    "\n",
    "def _softmax(x: np.ndarray, temp: float = 1.0) -> np.ndarray:\n",
    "    x = np.array(x, dtype=float) / max(temp, 1e-9)\n",
    "    x = x - np.max(x)\n",
    "    e = np.exp(x)\n",
    "    return e / np.maximum(e.sum(), 1e-12)\n",
    "\n",
    "\n",
    "class MVTAgent:\n",
    "    \"\"\"\n",
    "    Bayesian Forager (CSV-logging version):\n",
    "      - Movement restricted to env['visited_by_explorer'] == True (opened tiles).\n",
    "      - Mines are diggable iff digs_remaining > 0.\n",
    "      - Stay vs leave: SOFTMAX over [Q_stay, Q_leave] where\n",
    "            Q_stay  = E[r_next | this cell] - dig_cost\n",
    "            Q_leave = baseline - move_cost, baseline in {'env', 'rate'}\n",
    "      - Neighbor move: SOFTMAX over beta*label_score + (1-beta)*E[r_next | neighbor].\n",
    "      - Reward observations per cell are stored in r_obs_map[(r,c)] for Bayesian updating.\n",
    "      - Revealed types (Explorer Label) bypass inference via label_to_type.\n",
    "      - Use run(max_steps, csv_path=\"forager.csv\") to dump a step-by-step log to CSV.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_df: pd.DataFrame, cfg: MVTConfig, seed: Optional[int] = None):\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(seed if seed is not None else None)\n",
    "\n",
    "        self.env = ensure_runtime_columns(env_df)  # same reference\n",
    "        _indexify(self.env)\n",
    "\n",
    "        self.resource = cfg.init_resource\n",
    "        size = int(self.env.index.get_level_values(0).max()) + 1\n",
    "        self.base = (size // 2, size // 2)\n",
    "        self.pos = self.base\n",
    "        self.left_base_once = False\n",
    "\n",
    "        self.total_reward = 0.0\n",
    "        self.total_digs = 0\n",
    "        self.t = 0\n",
    "        self.log: List[Dict] = []\n",
    "\n",
    "        # Bayesian memory per cell\n",
    "        self.r_obs_map: Dict[Tuple[int, int], List[float]] = {}   # observed rewards\n",
    "        self.post_map: Dict[Tuple[int, int], np.ndarray] = {}     # cached P(z|data)\n",
    "\n",
    "        # Running average (if using rate baseline)\n",
    "        self.lambda_rate = 0.0\n",
    "\n",
    "        # Precompute env baseline: sum_k PI[k] * MU[k]\n",
    "        self.R_ENV = float(sum(p * m for p, m in zip(self.cfg.PI, self.cfg.MU)))\n",
    "\n",
    "        self.Nrows = int(self.env.index.get_level_values(0).max()) + 1\n",
    "        self.Ncols = int(self.env.index.get_level_values(1).max()) + 1\n",
    "\n",
    "        # Animation buffers\n",
    "        self.frames_intensity: List[np.ndarray] = []\n",
    "        self.frames_has_mines: List[np.ndarray] = []\n",
    "        self.frames_pos: List[Tuple[int, int]] = []\n",
    "        self.frames_reward: List[float] = []\n",
    "        self.frames_resource: List[float] = []\n",
    "        self.frames_action: List[str] = []\n",
    "        self.frames_decision: List[str] = []\n",
    "\n",
    "        self._log_enter()\n",
    "        self._snapshot_grid_state(action_label=\"start\", decision_label=\"starting\")\n",
    "\n",
    "    # ---------- env helpers ----------\n",
    "    def _cell_overall(self, pos=None):\n",
    "        if pos is None: pos = self.pos\n",
    "        return self.env.loc[pos, 'Overall Richness of this environment']  # unchanged\n",
    "\n",
    "    def _available_mines(self, pos=None) -> List[int]:\n",
    "        if pos is None: pos = self.pos\n",
    "        mines = []\n",
    "        for i in (1, 2, 3):\n",
    "            rem = self.env.loc[pos, f'Mine {i} digs_remaining']\n",
    "            cat = self.env.loc[pos, f'Number {i} mines']\n",
    "            if pd.notna(rem) and float(rem) > 0 and (cat is not None):\n",
    "                mines.append(i)\n",
    "        return mines\n",
    "\n",
    "    def _neighbors(self):\n",
    "        r, c = self.pos\n",
    "        neigh = [(r-1, c), (r+1, c), (r, c-1), (r, c+1)]\n",
    "        valid = [(rr, cc) for rr, cc in neigh if (rr, cc) in self.env.index]\n",
    "        if self.cfg.avoid_base and self.left_base_once:\n",
    "            valid = [p for p in valid if p != self.base]\n",
    "        # only tiles opened by explorer\n",
    "        valid = [p for p in valid if bool(self.env.loc[p, 'visited_by_explorer'])]\n",
    "        return valid\n",
    "\n",
    "    # ---------- Bayesian core ----------\n",
    "    def _type_index_from_label(self, label: Optional[str]) -> Optional[int]:\n",
    "        if label is None or (isinstance(label, float) and pd.isna(label)):\n",
    "            return None\n",
    "        return self.cfg.label_to_type.get(str(label).lower(), None)\n",
    "\n",
    "    def _r_list(self, pos: Tuple[int, int]) -> List[float]:\n",
    "        return self.r_obs_map.setdefault(pos, [])\n",
    "\n",
    "    def _loglike_sum_normal(self, r_list: List[float], mu: float, sigma2: float) -> float:\n",
    "        if not r_list:\n",
    "            return 0.0\n",
    "        n = len(r_list)\n",
    "        diffsq = sum((r - mu) * (r - mu) for r in r_list)\n",
    "        return -0.5 * n * np.log(2.0 * np.pi * sigma2) - 0.5 * (diffsq / sigma2)\n",
    "\n",
    "    def _posterior_over_types(self, pos: Tuple[int, int]) -> np.ndarray:\n",
    "        label = self.env.loc[pos, 'Explorer Label']\n",
    "        k_revealed = self._type_index_from_label(label)\n",
    "        if k_revealed is not None:\n",
    "            post = np.zeros(self.cfg.K, dtype=float)\n",
    "            post[k_revealed] = 1.0\n",
    "            self.post_map[pos] = post\n",
    "            return post\n",
    "\n",
    "        r_list = self._r_list(pos)\n",
    "        if len(r_list) == 0:\n",
    "            post = np.array(self.cfg.PI, dtype=float)\n",
    "            self.post_map[pos] = post\n",
    "            return post\n",
    "\n",
    "        logs = []\n",
    "        for k in range(self.cfg.K):\n",
    "            L = np.log(self.cfg.PI[k]) + self._loglike_sum_normal(r_list, self.cfg.MU[k], self.cfg.SIGMA2)\n",
    "            logs.append(L)\n",
    "        logs = np.array(logs, dtype=float)\n",
    "        m = logs.max()\n",
    "        w = np.exp(logs - m)\n",
    "        post = w / np.maximum(w.sum(), 1e-12)\n",
    "        self.post_map[pos] = post\n",
    "        return post\n",
    "\n",
    "    def _E_next_reward(self, pos: Tuple[int, int]) -> float:\n",
    "        if len(self._available_mines(pos)) == 0:\n",
    "            return 0.0\n",
    "        label = self.env.loc[pos, 'Explorer Label']\n",
    "        k_revealed = self._type_index_from_label(label)\n",
    "        if k_revealed is not None:\n",
    "            return float(self.cfg.MU[k_revealed])\n",
    "        post = self._posterior_over_types(pos)\n",
    "        return float(np.dot(post, np.array(self.cfg.MU, dtype=float)))\n",
    "\n",
    "    # ---------- values ----------\n",
    "    def _Q_stay(self) -> float:\n",
    "        E_next = self._E_next_reward(self.pos)\n",
    "        return E_next - float(self.cfg.dig_cost)*float(self.cfg.cost_sensitive_index)\n",
    "\n",
    "    def _Q_leave(self) -> float:\n",
    "        if self.cfg.baseline_mode == \"rate\":\n",
    "            baseline = self.lambda_rate\n",
    "        else:\n",
    "            baseline = self.R_ENV\n",
    "        return baseline - float(self.cfg.move_cost)*float(self.cfg.cost_sensitive_index)\n",
    "\n",
    "    # ---------- rich CSV logger ----------\n",
    "    def _log_step(\n",
    "        self,\n",
    "        *,\n",
    "        action: str,\n",
    "        decision: str,\n",
    "        resource_before: float,\n",
    "        resource_after: float,\n",
    "        Q_stay: Optional[float],\n",
    "        Q_leave: Optional[float],\n",
    "        stay_prob: Optional[float],\n",
    "        can_dig: Optional[bool],\n",
    "        can_move: Optional[bool],\n",
    "        # action-specific (dig)\n",
    "        mine_id: Optional[int] = None,\n",
    "        success: Optional[bool] = None,\n",
    "        reward_gained: Optional[float] = None,\n",
    "        depleted: Optional[bool] = None,\n",
    "        remaining_after: Optional[int] = None,\n",
    "        # action-specific (move)\n",
    "        move_choice_label: Optional[str] = None,\n",
    "        move_probs: Optional[List[float]] = None,\n",
    "        move_label_scores: Optional[List[float]] = None,\n",
    "        move_e_next: Optional[List[float]] = None\n",
    "    ):\n",
    "        # Posterior at current cell (after any updates already applied for this tick)\n",
    "        post = self.post_map.get(self.pos, None)\n",
    "        post_cols = {}\n",
    "        if post is not None:\n",
    "            for k in range(self.cfg.K):\n",
    "                post_cols[f\"post_k{k}\"] = float(post[k])\n",
    "\n",
    "        row = {\n",
    "            # time & pos\n",
    "            \"step\": int(self.t),\n",
    "            \"row\": int(self.pos[0]),\n",
    "            \"col\": int(self.pos[1]),\n",
    "            \"action\": action,\n",
    "            \"decision\": decision,\n",
    "\n",
    "            # resources\n",
    "            \"resource_before\": float(resource_before),\n",
    "            \"resource_after\": float(resource_after),\n",
    "\n",
    "            # values\n",
    "            \"Q_stay\": (None if Q_stay is None else float(Q_stay)),\n",
    "            \"Q_leave\": (None if Q_leave is None else float(Q_leave)),\n",
    "            \"stay_prob\": (None if stay_prob is None else float(stay_prob)),\n",
    "            \"E_next_here\": float(self._E_next_reward(self.pos)),\n",
    "\n",
    "            # feasibility\n",
    "            \"can_dig\": (None if can_dig is None else bool(can_dig)),\n",
    "            \"can_move\": (None if can_move is None else bool(can_move)),\n",
    "\n",
    "            # mined counts at current cell\n",
    "            \"available_mines_here\": int(len(self._available_mines(self.pos))),\n",
    "\n",
    "            # labels\n",
    "            \"overall_label\": str(self._cell_overall()),\n",
    "            \"explorer_label\": str(self.env.loc[self.pos, 'Explorer Label']),\n",
    "\n",
    "            # dig specifics\n",
    "            \"mine_id\": (None if mine_id is None else int(mine_id)),\n",
    "            \"dig_success\": (None if success is None else bool(success)),\n",
    "            \"reward_gained\": (None if reward_gained is None else float(reward_gained)),\n",
    "            \"depleted\": (None if depleted is None else bool(depleted)),\n",
    "            \"remaining_after\": (None if remaining_after is None else int(remaining_after)),\n",
    "\n",
    "            # move specifics (lists converted to repr later when saving)\n",
    "            \"move_choice_label\": (None if move_choice_label is None else str(move_choice_label)),\n",
    "            \"move_probs\": (None if move_probs is None else list(move_probs)),\n",
    "            \"move_label_scores\": (None if move_label_scores is None else list(move_label_scores)),\n",
    "            \"move_e_next\": (None if move_e_next is None else list(move_e_next)),\n",
    "\n",
    "            # running totals\n",
    "            \"reward_total\": float(self.total_reward),\n",
    "            \"total_digs\": int(self.total_digs),\n",
    "\n",
    "            # config snapshot\n",
    "            \"cfg_init_resource\": int(self.cfg.init_resource),\n",
    "            \"cfg_move_cost\": int(self.cfg.move_cost),\n",
    "            \"cfg_dig_cost\": int(self.cfg.dig_cost),\n",
    "            \"cfg_reward_amount\": float(self.cfg.reward_amount),\n",
    "            \"cfg_beta_trust\": float(self.cfg.beta_trust),\n",
    "            \"cfg_stay_leave_temp\": float(self.cfg.stay_leave_temp),\n",
    "            \"cfg_move_temp\": float(self.cfg.move_temp),\n",
    "            \"cfg_K\": int(self.cfg.K),\n",
    "            \"cfg_SIGMA2\": float(self.cfg.SIGMA2),\n",
    "            \"cfg_baseline_mode\": str(self.cfg.baseline_mode),\n",
    "            \"lambda_rate\": float(self.lambda_rate),\n",
    "            \"R_ENV\": float(self.R_ENV),\n",
    "        }\n",
    "        row.update(post_cols)\n",
    "        self.log.append(row)\n",
    "\n",
    "    # ---------- legacy lightweight log (kept for compatibility with your animations) ----------\n",
    "    def _log(self, **kw):\n",
    "        # Keep the old lightweight logs for any downstream animation code you already have.\n",
    "        kw.setdefault('step', self.t)\n",
    "        kw.setdefault('row', self.pos[0]); kw.setdefault('col', self.pos[1])\n",
    "        kw.setdefault('resource', self.resource)\n",
    "        kw.setdefault('reward_total', self.total_reward)\n",
    "        kw.setdefault('total_digs', self.total_digs)\n",
    "        kw.setdefault('overall', self._cell_overall())\n",
    "        kw.setdefault('E_next_here', self._E_next_reward(self.pos))\n",
    "        post = self.post_map.get(self.pos, None)\n",
    "        if post is not None:\n",
    "            for k in range(self.cfg.K):\n",
    "                kw.setdefault(f'post_k{k}', float(post[k]))\n",
    "        self.log.append(kw)\n",
    "\n",
    "    def _log_enter(self):\n",
    "        # Minimal enter-log for animation compatibility; CSV row is emitted on first decision step\n",
    "        self._log(action='enter', decision=\"enter\", v_stay=None, v_leave=None, note='entered_cell')\n",
    "\n",
    "    def _update_cell_empty_flag(self, pos=None):\n",
    "        if pos is None: pos = self.pos\n",
    "        has_any = len(self._available_mines(pos)) > 0\n",
    "        self.env.loc[pos, 'env_empty'] = (not has_any)\n",
    "\n",
    "    def total_food_left(self) -> float:\n",
    "        # Return how much of the forager's allocated food remains + reward.\n",
    "        return float(self.resource + self.total_reward)\n",
    "\n",
    "    # ---------- depletion snapshots for animation ----------\n",
    "    def _snapshot_grid_state(self, action_label: str = \"\", decision_label: str = \"\"):\n",
    "        MAX_DIGS_PER_CELL = 18\n",
    "        inten = np.zeros((self.Nrows, self.Ncols), dtype=float)\n",
    "        mask  = np.zeros((self.Nrows, self.Ncols), dtype=bool)\n",
    "        for (r, c) in self.env.index:\n",
    "            total = _cell_total_remaining_digs_df(self.env, (r, c))\n",
    "            inten[r, c] = min(total / MAX_DIGS_PER_CELL, 1.0)\n",
    "            mask[r, c] = (total > 0)\n",
    "\n",
    "        self.frames_intensity.append(inten)\n",
    "        self.frames_has_mines.append(mask)\n",
    "        self.frames_pos.append(tuple(self.pos))\n",
    "        self.frames_reward.append(float(self.total_reward))\n",
    "        self.frames_resource.append(float(self.resource))\n",
    "        self.frames_action.append(action_label if action_label else \"\")\n",
    "        self.frames_decision.append(decision_label if decision_label else \"\")\n",
    "\n",
    "    def _post_action_snapshot(self, action_label: str, decision_label: str):\n",
    "        self._snapshot_grid_state(action_label=action_label, decision_label=decision_label)\n",
    "\n",
    "    # ---------- actions ----------\n",
    "    def _auto_leave_if_empty(self) -> bool:\n",
    "        if len(self._available_mines()) > 0:\n",
    "            return False\n",
    "        if self.resource < self.cfg.move_cost or len(self._neighbors()) == 0:\n",
    "            # emit CSV row (halt)\n",
    "            self._log_step(\n",
    "                action='halt', decision='stuck_no_mines',\n",
    "                resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                Q_stay=None, Q_leave=None, stay_prob=None,\n",
    "                can_dig=False, can_move=False\n",
    "            )\n",
    "            self._post_action_snapshot(\"halt\", \"stuck_no_mines\")\n",
    "            return True\n",
    "        self._move(auto_leave=True)\n",
    "        return True\n",
    "\n",
    "    def _dig(self):\n",
    "        if self.resource < self.cfg.dig_cost:\n",
    "            self._log_step(\n",
    "                action='halt', decision='no_resource_to_dig',\n",
    "                resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                Q_stay=self._Q_stay(), Q_leave=self._Q_leave(), stay_prob=None,\n",
    "                can_dig=False, can_move=(len(self._neighbors()) > 0)\n",
    "            )\n",
    "            self._post_action_snapshot(\"halt\",\"no_resources_to_dig\")\n",
    "            return False\n",
    "\n",
    "        mines = self._available_mines()\n",
    "        if not mines:\n",
    "            return False\n",
    "\n",
    "        # Mine choice within the cell (kept compatible with your earlier heuristic)\n",
    "        vals = []\n",
    "        for i in mines:\n",
    "            cat = self.env.loc[self.pos, f'Number {i} mines']\n",
    "            v = self.cfg.mine_choice_value.get(cat, 0.5)\n",
    "            vals.append(max(v, 1e-9))\n",
    "        w = np.array(vals, dtype=float)\n",
    "        mine_id = int(self.rng.choice(mines, p=w / w.sum()))\n",
    "\n",
    "        # Dig mechanics\n",
    "        resource_before = float(self.resource)\n",
    "        p = self.env.loc[self.pos, f'Mine {mine_id} reward_prob']\n",
    "        self.resource -= self.cfg.dig_cost\n",
    "        success = (p is not None) and (self.rng.random() < float(p))\n",
    "        r_obs = (self.cfg.reward_amount if success else 0.0)\n",
    "\n",
    "        # Update totals and Bayesian memory\n",
    "        if success:\n",
    "            self.total_reward += self.cfg.reward_amount\n",
    "        self.total_digs += 1\n",
    "        self._r_list(self.pos).append(float(r_obs))      # store observation for posterior\n",
    "        self._posterior_over_types(self.pos)             # refresh cached posterior\n",
    "\n",
    "        # Deplete mine\n",
    "        rem_col = f\"Mine {mine_id} digs_remaining\"\n",
    "        new_rem = int(self.env.loc[self.pos, rem_col]) - 1\n",
    "        self.env.loc[self.pos, rem_col] = new_rem\n",
    "        depleted = (new_rem <= 0)\n",
    "        if depleted:\n",
    "            self.env.loc[self.pos, f'Number {mine_id} mines'] = None\n",
    "            self.env.loc[self.pos, f'Mine {mine_id} reward_prob'] = None\n",
    "            self.env.loc[self.pos, rem_col] = None\n",
    "\n",
    "        # Update running λ if using rate baseline\n",
    "        if self.cfg.baseline_mode == \"rate\":\n",
    "            g = r_obs - float(self.cfg.dig_cost)\n",
    "            self.lambda_rate = (1.0 - self.cfg.ewma_eta) * self.lambda_rate + self.cfg.ewma_eta * g\n",
    "\n",
    "        # Emit CSV row\n",
    "        self._log_step(\n",
    "            action='dig', decision='stay',\n",
    "            resource_before=resource_before, resource_after=float(self.resource),\n",
    "            Q_stay=self._Q_stay(), Q_leave=self._Q_leave(),\n",
    "            stay_prob=None,  # only relevant at choice time; here we already chose\n",
    "            can_dig=True, can_move=(len(self._neighbors()) > 0),\n",
    "            mine_id=mine_id, success=bool(success), reward_gained=float(r_obs),\n",
    "            depleted=bool(depleted), remaining_after=(None if depleted else new_rem)\n",
    "        )\n",
    "        self._post_action_snapshot(\"dig\",\"stay\")\n",
    "        return True\n",
    "\n",
    "    def _move(self, auto_leave: bool = False):\n",
    "        if self.resource < self.cfg.move_cost:\n",
    "            self._log_step(\n",
    "                action='halt', decision='no_resource_to_move',\n",
    "                resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                Q_stay=self._Q_stay(), Q_leave=self._Q_leave(), stay_prob=None,\n",
    "                can_dig=(len(self._available_mines()) > 0), can_move=False\n",
    "            )\n",
    "            self._post_action_snapshot(\"halt\",\"no_resource_to_move\")\n",
    "            return False\n",
    "\n",
    "        neigh = self._neighbors()\n",
    "        if not neigh:\n",
    "            self._log_step(\n",
    "                action='halt', decision='no_neighbors',\n",
    "                resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                Q_stay=self._Q_stay(), Q_leave=self._Q_leave(), stay_prob=None,\n",
    "                can_dig=(len(self._available_mines()) > 0), can_move=False\n",
    "            )\n",
    "            self._post_action_snapshot(\"halt\",\"no_neighbors\")\n",
    "            return False\n",
    "\n",
    "        # Neighbor softmax: beta * label_score + (1-beta) * E_next(neighbor)\n",
    "        beta = float(self.cfg.beta_trust)\n",
    "\n",
    "        labels = [self.env.loc[n, 'Explorer Label'] for n in neigh]\n",
    "        label_scores = np.array([self.cfg.label_value.get(str(l).lower(), 0.5) if pd.notna(l) else 0.5\n",
    "                                 for l in labels], dtype=float)\n",
    "\n",
    "        # Bayesian expected next reward per neighbor\n",
    "        e_next = np.array([self._E_next_reward(n) for n in neigh], dtype=float)\n",
    "\n",
    "        # Normalize e_next to a comparable 0–1 scale for mixing (avoid magnitude mismatch)\n",
    "        scale = max(self.cfg.reward_amount, 1e-6)\n",
    "        e_scaled = np.clip(e_next / scale, 0.0, 1.0)\n",
    "\n",
    "        logits = beta * label_scores + (1.0 - beta) * e_scaled\n",
    "        probs = _softmax(logits, temp=self.cfg.move_temp)\n",
    "\n",
    "        choice_idx = int(self.rng.choice(len(neigh), p=probs))\n",
    "        choice = neigh[choice_idx]\n",
    "        choice_label = labels[choice_idx]\n",
    "\n",
    "        # Pay and move\n",
    "        resource_before = float(self.resource)\n",
    "        self.resource -= self.cfg.move_cost\n",
    "        if (self.pos == self.base) and (choice != self.base):\n",
    "            self.left_base_once = True\n",
    "        self.pos = choice\n",
    "\n",
    "        # Update running λ if using rate baseline\n",
    "        if self.cfg.baseline_mode == \"rate\":\n",
    "            g = -float(self.cfg.move_cost)\n",
    "            self.lambda_rate = (1.0 - self.cfg.ewma_eta) * self.lambda_rate + self.cfg.ewma_eta * g\n",
    "\n",
    "        # Emit CSV row\n",
    "        self._log_step(\n",
    "            action='move', decision=('auto_leave' if auto_leave else 'leave'),\n",
    "            resource_before=resource_before, resource_after=float(self.resource),\n",
    "            Q_stay=self._Q_stay(), Q_leave=self._Q_leave(),\n",
    "            stay_prob=None,  # relevant at stay/leave choice time\n",
    "            can_dig=(len(self._available_mines()) > 0), can_move=True,\n",
    "            move_choice_label=choice_label,\n",
    "            move_probs=list(np.round(probs, 4)),\n",
    "            move_label_scores=list(np.round(label_scores, 3)),\n",
    "            move_e_next=list(np.round(e_next, 3))\n",
    "        )\n",
    "        self._post_action_snapshot(\"move\", \"auto_leave\" if auto_leave else \"leave\")\n",
    "        self._log_enter()\n",
    "        return True\n",
    "\n",
    "    # ---------- main loop (softmax stay vs leave) ----------\n",
    "    def step(self):\n",
    "        # Auto-leave if no mines here but movement is possible\n",
    "        if self._auto_leave_if_empty():\n",
    "            self.t += 1\n",
    "            return\n",
    "\n",
    "        can_dig = (self.resource >= self.cfg.dig_cost) and (len(self._available_mines()) > 0)\n",
    "        can_move = (self.resource >= self.cfg.move_cost) and (len(self._neighbors()) > 0)\n",
    "\n",
    "        if not can_dig and not can_move:\n",
    "            self._log_step(\n",
    "                action='halt', decision='insufficient_actions',\n",
    "                resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                Q_stay=None, Q_leave=None, stay_prob=None,\n",
    "                can_dig=False, can_move=False\n",
    "            )\n",
    "            self._post_action_snapshot(\"halt\",\"insufficient_actions\")\n",
    "            self.t += 1\n",
    "            return\n",
    "\n",
    "        Q_stay, Q_leave = self._Q_stay(), self._Q_leave()\n",
    "        logits = np.array([\n",
    "            Q_stay if can_dig else -1e9,\n",
    "            Q_leave if can_move else -1e9\n",
    "        ], dtype=float)\n",
    "        probs = _softmax(logits, temp=self.cfg.stay_leave_temp)\n",
    "        p_stay = float(probs[0])\n",
    "\n",
    "        resource_before = float(self.resource)\n",
    "        choice = int(self.rng.choice([0, 1], p=probs))  # 0=stay(dig), 1=leave(move)\n",
    "\n",
    "        if choice == 0 and can_dig:\n",
    "            self._dig()\n",
    "        elif choice == 1 and can_move:\n",
    "            self._move(auto_leave=False)\n",
    "        else:\n",
    "            # fallback\n",
    "            if can_dig:\n",
    "                self._dig()\n",
    "            else:\n",
    "                self._move(auto_leave=False)\n",
    "\n",
    "        # Log a high-level \"decision row\" for this tick if you want *both* the decision\n",
    "        # and the action rows. If you prefer only action rows, comment this block out.\n",
    "        self._log_step(\n",
    "            action=\"decide\", decision=(\"stay\" if (choice == 0 and can_dig) else \"leave\"),\n",
    "            resource_before=resource_before, resource_after=float(self.resource),\n",
    "            Q_stay=Q_stay, Q_leave=Q_leave, stay_prob=p_stay,\n",
    "            can_dig=can_dig, can_move=can_move\n",
    "        )\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "    def run(self, max_steps: int = 300, csv_path: Optional[str] = None) -> pd.DataFrame:\n",
    "        for _ in range(max_steps):\n",
    "            # stop if can't do anything\n",
    "            if (self.resource < self.cfg.dig_cost) and (self.resource < self.cfg.move_cost):\n",
    "                self._log_step(\n",
    "                    action='halt', decision='insufficient_resources',\n",
    "                    resource_before=float(self.resource), resource_after=float(self.resource),\n",
    "                    Q_stay=None, Q_leave=None, stay_prob=None,\n",
    "                    can_dig=False, can_move=False\n",
    "                )\n",
    "                self._post_action_snapshot(\"halt\", \"insufficient_resources\")\n",
    "                break\n",
    "            self.step()\n",
    "            # If last *action* row was a hard halt, we can break\n",
    "            if self.log and isinstance(self.log[-1], dict) and self.log[-1].get('action') == 'halt':\n",
    "                break\n",
    "\n",
    "        df = pd.DataFrame(self.log)\n",
    "        if csv_path:\n",
    "            # Convert list-like columns to safe strings\n",
    "            for col in (\"move_probs\", \"move_label_scores\", \"move_e_next\"):\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].apply(lambda v: (\"\" if v is None else repr(v)))\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616de75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Animation helpers -------------------\n",
    "def _env_index_or_copy(env: pd.DataFrame) -> pd.DataFrame:\n",
    "    if list(env.index.names) == [\"Row\", \"Col\"]:\n",
    "        return env\n",
    "    return env.set_index([\"Row\", \"Col\"], drop=False)\n",
    "\n",
    "def animate_explorer(env: pd.DataFrame, agent: ExplorerAgent, outpath: str = \"explore_animation.gif\"):\n",
    "    \"\"\"\n",
    "    Colors the grid by TRUE richness (no 'unknown' blocks).\n",
    "    Dim tiles that still contain any HIDDEN mines (based on per-mine 'revealed' flags).\n",
    "    \"\"\"\n",
    "    frames = len(agent.frames_unrevealed_mask)\n",
    "    pos_seq = agent.frames_pos\n",
    "    resource_seq = agent.frames_resource\n",
    "    action_seq = agent.frames_action\n",
    "    decision_seq = agent.frames_decision\n",
    "\n",
    "    env_idx = _env_index_or_copy(env)\n",
    "    Nrows = int(env_idx.index.get_level_values(0).max()) + 1\n",
    "    Ncols = int(env_idx.index.get_level_values(1).max()) + 1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    legend_patches = [mpatches.Patch(color=col, label=lab) for lab, col in RICHNESS_COLORS.items()]\n",
    "    ax.legend(handles=legend_patches, loc=\"upper right\", title=\"Richness (TRUE for explorer)\")\n",
    "\n",
    "    tiles = {}\n",
    "    # Create tiles colored by TRUE richness\n",
    "    for (r, c), cell in env_idx.iterrows():\n",
    "        richness_true = cell[\"TRUE Overall Richness\"]\n",
    "        base_rgb = np.array(mcolors.to_rgb(RICHNESS_COLORS.get(richness_true, \"white\")))\n",
    "        rect = plt.Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor=base_rgb, edgecolor=\"black\")\n",
    "        ax.add_patch(rect)\n",
    "        tiles[(r, c)] = rect\n",
    "\n",
    "    # Axes/layout\n",
    "    ax.set_xlim(-0.5, Ncols - 0.5)\n",
    "    ax.set_ylim(Nrows - 0.5, -0.5)\n",
    "    ax.set_xticks(range(Ncols))\n",
    "    ax.set_yticks(range(Nrows))\n",
    "    ax.grid(True, linestyle=\":\", linewidth=0.5)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(\"Explorer (TRUE map color; dim = has hidden mines)\")\n",
    "\n",
    "    # Agent marker & HUD\n",
    "    agent_dot, = ax.plot([], [], \"bo\", markersize=10)\n",
    "    hud_text = ax.text(0.02, 0.98, \"\", transform=ax.transAxes, va='top', ha='left')\n",
    "\n",
    "    def init_anim():\n",
    "        agent_dot.set_data([], [])\n",
    "        hud_text.set_text(\"\")\n",
    "        return [agent_dot, hud_text] + list(tiles.values())\n",
    "\n",
    "    def update(frame):\n",
    "        if frame >= frames:\n",
    "            frame = frames - 1\n",
    "\n",
    "        # Keep TRUE richness color each frame\n",
    "        for (r, c), rect in tiles.items():\n",
    "            richness_true = env_idx.loc[(r, c), \"TRUE Overall Richness\"]\n",
    "            rect.set_facecolor(mcolors.to_rgb(RICHNESS_COLORS.get(richness_true, \"white\")))\n",
    "\n",
    "        # Dim tiles that still have any hidden mine slots (from agent snapshot)\n",
    "        mask = agent.frames_unrevealed_mask[frame]\n",
    "        for (r, c), rect in tiles.items():\n",
    "            if mask[r, c]:\n",
    "                col = np.array(rect.get_facecolor()[:3])\n",
    "                rect.set_facecolor(np.clip(col * 0.6, 0, 1))\n",
    "\n",
    "        rr, cc = pos_seq[frame]\n",
    "        agent_dot.set_data([cc], [rr])\n",
    "        hud_text.set_text(f\"t={frame}  action={action_seq[frame]}\\nresource={resource_seq[frame]:.2f}\\ndecision={decision_seq[frame]}\")\n",
    "        return [agent_dot, hud_text] + list(tiles.values())\n",
    "\n",
    "    anim = FuncAnimation(fig, update, init_func=init_anim, frames=frames, interval=150, blit=True, repeat=False)\n",
    "    anim.save(outpath, writer=PillowWriter(fps=6))\n",
    "    plt.close(fig)\n",
    "\n",
    "def animate_forager(env: pd.DataFrame, agent: MVTAgent, outpath: str = \"forage_animation.gif\"):\n",
    "    \"\"\"\n",
    "    Uses the *live* shared env to recolor each frame from the current visible label\n",
    "    ('Overall Richness of this environment'), so explorer’s auto-labeling / reveals are shown.\n",
    "    Dimming reflects remaining digs intensity from the agent’s snapshots.\n",
    "    \"\"\"\n",
    "    frames = len(agent.frames_intensity)\n",
    "    pos_seq = agent.frames_pos\n",
    "    reward_seq = agent.frames_reward\n",
    "    resource_seq = agent.frames_resource\n",
    "    action_seq = agent.frames_action\n",
    "    decision_seq = agent.frames_decision\n",
    "\n",
    "    env_idx = _env_index_or_copy(env)\n",
    "    Nrows = int(env_idx.index.get_level_values(0).max()) + 1\n",
    "    Ncols = int(env_idx.index.get_level_values(1).max()) + 1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    legend_patches = [mpatches.Patch(color=col, label=lab) for lab, col in RICHNESS_COLORS.items()]\n",
    "    ax.legend(handles=legend_patches, loc=\"upper right\", title=\"Richness (visible)\")\n",
    "\n",
    "    tiles = {}\n",
    "    # Create rectangles; we'll recolor every frame from agent.env\n",
    "    for (r, c), _cell in env_idx.iterrows():\n",
    "        rect = plt.Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor=\"white\", edgecolor=\"black\")\n",
    "        ax.add_patch(rect)\n",
    "        tiles[(r, c)] = rect\n",
    "\n",
    "    # Axes/layout\n",
    "    ax.set_xlim(-0.5, Ncols - 0.5)\n",
    "    ax.set_ylim(Nrows - 0.5, -0.5)\n",
    "    ax.set_xticks(range(Ncols))\n",
    "    ax.set_yticks(range(Nrows))\n",
    "    ax.grid(True, linestyle=\":\", linewidth=0.5)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(\"Forager (action & reward)\")\n",
    "\n",
    "    # Agent marker & HUD\n",
    "    agent_dot, = ax.plot([], [], \"ro\", markersize=12)\n",
    "    hud_text = ax.text(0.02, 0.98, \"\", transform=ax.transAxes, va='top', ha='left')\n",
    "\n",
    "    def init_anim():\n",
    "        agent_dot.set_data([], [])\n",
    "        hud_text.set_text(\"\")\n",
    "        return [agent_dot, hud_text] + list(tiles.values())\n",
    "\n",
    "    def update(frame):\n",
    "        if frame >= frames:\n",
    "            frame = frames - 1\n",
    "\n",
    "        inten = agent.frames_intensity[frame]   # [0..1] remaining digs normalized\n",
    "        hasm = agent.frames_has_mines[frame]    # bool mines-present mask\n",
    "\n",
    "        # Recolor from CURRENT visible label in the shared env, then dim by intensity\n",
    "        for (r, c), rect in tiles.items():\n",
    "            richness_vis = agent.env.loc[(r, c), \"Explorer Label\"]\n",
    "            base_rgb = np.array(mcolors.to_rgb(RICHNESS_COLORS.get(richness_vis, \"white\")))\n",
    "\n",
    "            if not hasm[r, c]:\n",
    "                scale = 0.20  # depleted: keep hue but very dim\n",
    "            else:\n",
    "                scale = 0.30 + 0.70 * float(inten[r, c])\n",
    "            rect.set_facecolor(np.clip(base_rgb * scale, 0, 1))\n",
    "\n",
    "        rr, cc = pos_seq[frame]\n",
    "        rr, cc = int(rr), int(cc)\n",
    "        agent_dot.set_data([cc], [rr])\n",
    "        hud_text.set_text(\n",
    "            f\"t={frame}  action={action_seq[frame]}\\n\"\n",
    "            f\"reward_total={reward_seq[frame]:.2f}  resource={resource_seq[frame]:.2f}  decision={decision_seq[frame]}\"\n",
    "        )\n",
    "        return [agent_dot, hud_text] + list(tiles.values())\n",
    "\n",
    "    anim = FuncAnimation(fig, update, init_func=init_anim, frames=frames, interval=150, blit=True, repeat=False)\n",
    "    anim.save(outpath, writer=PillowWriter(fps=6))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d3785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== ROUND 1 ==========\n",
      "[Leader] Allocation → Explorer: 200 | Forager: 200\n",
      "=== Exploration Summary ===\n",
      "Explorer steps: 10 | Remaining resource: 166.00 | Hidden mines left: 14\n",
      "=== Foraging Summary ===\n",
      "Total reward received: 1.00\n",
      "Resource remaining: 2.00\n",
      "[Leader] Next allocation planned → Explorer: 150 | Forager: 250\n",
      "\n",
      "========== ROUND 2 ==========\n",
      "[Leader] Allocation → Explorer: 150 | Forager: 250\n",
      "=== Exploration Summary ===\n",
      "Explorer steps: 9 | Remaining resource: 118.00 | Hidden mines left: 14\n",
      "=== Foraging Summary ===\n",
      "Total reward received: 0.00\n",
      "Resource remaining: 2.00\n",
      "[Leader] Next allocation planned → Explorer: 88 | Forager: 312\n",
      "\n",
      "========== ROUND 3 ==========\n",
      "[Leader] Allocation → Explorer: 88 | Forager: 312\n",
      "=== Exploration Summary ===\n",
      "Explorer steps: 10 | Remaining resource: 54.00 | Hidden mines left: 11\n",
      "=== Foraging Summary ===\n",
      "Total reward received: 6.00\n",
      "Resource remaining: 0.00\n",
      "[Leader] Next allocation planned → Explorer: 88 | Forager: 312\n",
      "\n",
      "========== ROUND 4 ==========\n",
      "[Leader] Allocation → Explorer: 88 | Forager: 312\n",
      "=== Exploration Summary ===\n",
      "Explorer steps: 10 | Remaining resource: 54.00 | Hidden mines left: 14\n",
      "=== Foraging Summary ===\n",
      "Total reward received: 3.00\n",
      "Resource remaining: 2.00\n",
      "[Leader] Next allocation planned → Explorer: 25 | Forager: 375\n",
      "\n",
      "========== ROUND 5 ==========\n",
      "[Leader] Allocation → Explorer: 25 | Forager: 375\n",
      "=== Exploration Summary ===\n",
      "Explorer steps: 7 | Remaining resource: 1.00 | Hidden mines left: 12\n",
      "=== Foraging Summary ===\n",
      "Total reward received: 0.00\n",
      "Resource remaining: 3.00\n",
      "[Leader] Next allocation planned → Explorer: 20 | Forager: 380\n",
      "\n",
      "Saved:\n",
      " - rounds_summary.csv\n",
      " - explorer_round1.csv, forager_round1.csv\n",
      " - explorer_round2.csv, forager_round2.csv\n",
      " - explorer_round3.csv, forager_round3.csv\n",
      " - explorer_round4.csv, forager_round4.csv\n",
      " - explorer_round5.csv, forager_round5.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    ROUNDS = 5\n",
    "\n",
    "    # --- Leader & global settings ---\n",
    "    ld_cfg = LeaderConfig(total_food=400, alpha=0.3)\n",
    "    leader = LeaderAgent(ld_cfg, seed=None)\n",
    "\n",
    "    # ---- Safe helpers for multi-round allocation ----\n",
    "    def _safe_total_food(ldr):\n",
    "        if hasattr(ldr, \"total_food\"):\n",
    "            return int(getattr(ldr, \"total_food\"))\n",
    "        cfg = getattr(ldr, \"cfg\", None)\n",
    "        if cfg is not None and hasattr(cfg, \"total_food\"):\n",
    "            return int(getattr(cfg, \"total_food\"))\n",
    "        raise AttributeError(\"LeaderAgent has neither 'total_food' nor 'cfg.total_food'.\")\n",
    "\n",
    "    def _safe_forager_share(ldr, default=0.5):\n",
    "        # Prefer explicit share attr if present\n",
    "        for name in (\"forager_share\", \"share_forager\", \"p_forager\"):\n",
    "            if hasattr(ldr, name):\n",
    "                try:\n",
    "                    val = float(getattr(ldr, name))\n",
    "                    return max(0.0, min(1.0, val))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # Or infer from a (explorer, forager) allocation getter\n",
    "        if hasattr(ldr, \"get_allocation\"):\n",
    "            try:\n",
    "                alloc = ldr.get_allocation()\n",
    "                if isinstance(alloc, (list, tuple)) and len(alloc) == 2:\n",
    "                    e_food, f_food = alloc\n",
    "                    tot = float(e_food) + float(f_food)\n",
    "                    if tot > 0:\n",
    "                        return float(f_food) / tot\n",
    "            except Exception:\n",
    "                pass\n",
    "        return default\n",
    "\n",
    "    def current_allocation(ldr):\n",
    "        \"\"\"\n",
    "        Determine (explorer_food, forager_food) for THIS round.\n",
    "        - If leader computed next budgets on the last update, use them.\n",
    "        - Else compute from total_food * forager_share (or default 50/50).\n",
    "        \"\"\"\n",
    "        # If previous update computed next integers, prefer those\n",
    "        ef_next = getattr(ldr, \"explorer_food_next\", None)\n",
    "        ff_next = getattr(ldr, \"forager_food_next\", None)\n",
    "        if isinstance(ef_next, (int, float)) and isinstance(ff_next, (int, float)):\n",
    "            return int(ef_next), int(ff_next)\n",
    "\n",
    "        total = _safe_total_food(ldr)\n",
    "        f_share = _safe_forager_share(ldr, 0.5)\n",
    "        f_food = int(round(total * f_share))\n",
    "        e_food = int(total - f_food)\n",
    "        return e_food, f_food\n",
    "\n",
    "    # --- Safe replacement for LeaderAgent.update_allocation; then bind it ---\n",
    "    def _safe_update_allocation(self):\n",
    "        \"\"\"\n",
    "        Safely update the forager/explorer resource split based on forager performance.\n",
    "        - Multiplicative update on forager_share with bounded change.\n",
    "        - No division-by-zero; stable when total_reward == 0.\n",
    "        \"\"\"\n",
    "        total_food = getattr(self, \"total_food\", None)\n",
    "        if total_food is None:\n",
    "            total_food = getattr(self.cfg, \"total_food\", None)\n",
    "        if total_food is None:\n",
    "            raise AttributeError(\"LeaderAgent must have total_food or cfg.total_food.\")\n",
    "\n",
    "        # Ensure persistent share; default 50/50\n",
    "        if not hasattr(self, \"forager_share\"):\n",
    "            self.forager_share = 0.5\n",
    "\n",
    "        f = getattr(self, \"forager\", None)\n",
    "        if f is None:\n",
    "            # Nothing to update yet\n",
    "            self.forager_food_next  = int(round(total_food * self.forager_share))\n",
    "            self.explorer_food_next = int(total_food - self.forager_food_next)\n",
    "            return\n",
    "\n",
    "        total_reward = float(getattr(f, \"total_reward\", 0.0))\n",
    "        perceived_reward = float(f.total_food_left())  # resource + reward (your method)\n",
    "        diff = perceived_reward - total_reward\n",
    "\n",
    "        denom = total_reward if total_reward > 0 else max(abs(perceived_reward), 1.0)\n",
    "        raw_change = float(self.cfg.alpha) * (diff / denom)\n",
    "\n",
    "        MAX_DELTA = 0.25  # cap ±25% change/round\n",
    "        change = max(-MAX_DELTA, min(MAX_DELTA, raw_change))\n",
    "\n",
    "        new_share = self.forager_share * (1.0 + change)\n",
    "        self.forager_share = float(max(0.05, min(0.95, new_share)))\n",
    "        self.explorer_share = 1.0 - self.forager_share\n",
    "\n",
    "        self.forager_food_next  = int(round(total_food * self.forager_share))\n",
    "        self.explorer_food_next = int(total_food - self.forager_food_next)\n",
    "\n",
    "    # Bind the method (monkey patch) BEFORE the loop\n",
    "    LeaderAgent.update_allocation = _safe_update_allocation\n",
    "\n",
    "    # --- Per-round summary rows will be appended here and saved at the end ---\n",
    "    rounds_rows = []\n",
    "\n",
    "    for r in range(1, ROUNDS + 1):\n",
    "        print(f\"\\n========== ROUND {r} ==========\")\n",
    "\n",
    "        # (Re)build env for this round (fresh ground truth & visible map)\n",
    "        env = init_gridworld(size=3, seed=None)\n",
    "\n",
    "        # --- Allocation for this round ---\n",
    "        e_food, f_food = current_allocation(leader)\n",
    "        print(f\"[Leader] Allocation → Explorer: {e_food} | Forager: {f_food}\")\n",
    "\n",
    "        # --- Phase 1: Explorer ---\n",
    "        ex_cfg = ExplorerConfig(\n",
    "            init_resource=e_food,\n",
    "            move_cost=4, scan_cost=2,\n",
    "            gamma=0.1, beta_local=0.6, beta_global=0.4,\n",
    "            avoid_base=True, no_backtrack=True\n",
    "        )\n",
    "        explorer = ExplorerAgent(env, ex_cfg, seed=None)\n",
    "\n",
    "        # Save full step log to CSV\n",
    "        ex_csv = f\"explorer_round{r}.csv\"\n",
    "        ex_traj = explorer.run(max_steps=300, csv_path=ex_csv)\n",
    "        animate_explorer(env, explorer, outpath=f\"explore_round{r}.gif\")\n",
    "\n",
    "        # --- Transfer to Forager (same DataFrame reference) ---\n",
    "        env_for_forager = explorer.export_env_for_forager()\n",
    "\n",
    "        # --- Phase 2: Forager (Bayesian) ---\n",
    "        fg_cfg = MVTConfig(\n",
    "            init_resource=f_food,\n",
    "            move_cost=4, dig_cost=2,\n",
    "            reward_amount=1.0,\n",
    "            # Bayesian params (set to your task)\n",
    "            K=3,\n",
    "            MU=[0.2, 0.6, 0.9],   # <- set to design/pilot expected rewards per dig\n",
    "            PI=[1/3, 1/3, 1/3],\n",
    "            SIGMA2=0.05,\n",
    "            label_to_type={'poor':0, 'neutral':1, 'rich':2},\n",
    "            # policy temps & trust\n",
    "            beta_trust=0.7, stay_leave_temp=0.2, move_temp=0.2, cost_sensitive_index=0.1\n",
    "            # leaving baseline\n",
    "            baseline_mode=\"env\"   # or \"rate\"\n",
    "        )\n",
    "        forager = MVTAgent(env_for_forager, fg_cfg, seed=None)\n",
    "\n",
    "        # Save full step log to CSV\n",
    "        fg_csv = f\"forager_round{r}.csv\"\n",
    "        traj = forager.run(max_steps=300, csv_path=fg_csv)\n",
    "        animate_forager(env_for_forager, forager, outpath=f\"forage_round{r}.gif\")\n",
    "\n",
    "        # --- Hand results to Leader and update allocation for NEXT round ---\n",
    "        # Keep a snapshot of share before update for the summary table\n",
    "        share_before = _safe_forager_share(leader, 0.5)\n",
    "        leader.set_forager(forager)\n",
    "        leader.update_allocation()\n",
    "        share_after = _safe_forager_share(leader, share_before)\n",
    "\n",
    "        # --- Round summary (append a row we’ll save later) ---\n",
    "        row_summary = {\n",
    "            \"round\": r,\n",
    "            # allocations used at start of this round\n",
    "            \"explorer_food_used\": int(e_food),\n",
    "            \"forager_food_used\": int(f_food),\n",
    "            \"forager_share_before\": float(share_before),\n",
    "            # explorer outcomes\n",
    "            \"explorer_steps\": int(len(ex_traj)),\n",
    "            \"explorer_resource_remaining\": float(explorer.resource),\n",
    "            \"hidden_mines_left\": int(explorer._global_hidden_count()),\n",
    "            # forager outcomes\n",
    "            \"forager_steps\": int(len(traj)),\n",
    "            \"forager_reward_total\": float(forager.total_reward),\n",
    "            \"forager_resource_remaining\": float(forager.resource),\n",
    "            \"forager_total_digs\": int(forager.total_digs),\n",
    "            # leader plan for next round\n",
    "            \"forager_share_after\": float(share_after),\n",
    "            \"next_explorer_food\": int(getattr(leader, \"explorer_food_next\", 0)),\n",
    "            \"next_forager_food\": int(getattr(leader, \"forager_food_next\", 0)),\n",
    "            # file artifacts\n",
    "            \"explorer_csv\": ex_csv,\n",
    "            \"forager_csv\": fg_csv,\n",
    "            \"explorer_gif\": f\"explore_round{r}.gif\",\n",
    "            \"forager_gif\": f\"forage_round{r}.gif\",\n",
    "        }\n",
    "        rounds_rows.append(row_summary)\n",
    "\n",
    "        # --- Console summaries (unchanged) ---\n",
    "        print(\"=== Exploration Summary ===\")\n",
    "        print(f\"Explorer steps: {len(ex_traj)} | Remaining resource: {explorer.resource:.2f} \"\n",
    "              f\"| Hidden mines left: {explorer._global_hidden_count()}\")\n",
    "        print(\"=== Foraging Summary ===\")\n",
    "        print(f\"Total reward received: {forager.total_reward:.2f}\")\n",
    "        print(f\"Resource remaining: {forager.resource:.2f}\")\n",
    "        print(f\"[Leader] Next allocation planned → Explorer: {getattr(leader, 'explorer_food_next', 'n/a')} | \"\n",
    "              f\"Forager: {getattr(leader, 'forager_food_next', 'n/a')}\")\n",
    "\n",
    "    # --- Save per-round summary table to CSV at the very end ---\n",
    "    pd.DataFrame(rounds_rows).to_csv(\"rounds_summary.csv\", index=False)\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" - rounds_summary.csv\")\n",
    "    for r in range(1, ROUNDS + 1):\n",
    "        print(f\" - explorer_round{r}.csv, forager_round{r}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
